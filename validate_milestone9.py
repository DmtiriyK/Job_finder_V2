#!/usr/bin/env python3
"""
Milestone 9 Acceptance Tests: Production Ready & Documented

Validates that the Job Finder project is production-ready with:
- Comprehensive documentation
- High code coverage (‚â•80% for core modules)
- All tests passing
- No credentials in code
- Performance benchmarks documented

Run: python validate_milestone9.py
"""

import subprocess
import sys
import os
from pathlib import Path
import json
from typing import Dict, List, Tuple


class Color:
    """ANSI color codes for terminal output"""
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'


def print_header(text: str):
    """Print formatted section header"""
    print(f"\n{Color.CYAN}{Color.BOLD}{'=' * 70}{Color.ENDC}")
    print(f"{Color.CYAN}{Color.BOLD}{text:^70}{Color.ENDC}")
    print(f"{Color.CYAN}{Color.BOLD}{'=' * 70}{Color.ENDC}\n")


def print_test(name: str, passed: bool, details: str = ""):
    """Print test result"""
    status = f"{Color.GREEN}‚úì PASS{Color.ENDC}" if passed else f"{Color.RED}‚úó FAIL{Color.ENDC}"
    print(f"{status} {name}")
    if details:
        print(f"      {Color.YELLOW}{details}{Color.ENDC}")


def check_file_exists(filepath: str) -> Tuple[bool, str]:
    """Check if file exists and return status"""
    path = Path(filepath)
    if path.exists():
        size = path.stat().st_size
        lines = len(path.read_text(encoding='utf-8').splitlines()) if size > 0 else 0
        return True, f"{lines} lines, {size} bytes"
    return False, "File not found"


def check_documentation_exists() -> bool:
    """Test 1: All documentation files exist"""
    print_header("Test 1: Documentation Files")
    
    required_docs = [
        ("README.md", 400),
        ("docs/GOOGLE_SHEETS_SETUP.md", 200),
        ("docs/GITHUB_ACTIONS_SETUP.md", 400),
        ("docs/CUSTOMIZATION.md", 500),
        ("docs/ADDING_SCRAPERS.md", 700),
        ("docs/TROUBLESHOOTING.md", 500),
        ("IMPLEMENTATION_PLAN.md", 100),
        ("MILESTONES.md", 800),
    ]
    
    all_exist = True
    for filepath, min_lines in required_docs:
        exists, details = check_file_exists(filepath)
        
        if exists:
            lines = int(details.split()[0])
            passed = lines >= min_lines
            status = f"Expected ‚â•{min_lines} lines, got {lines}"
            print_test(f"{filepath}", passed, status)
            if not passed:
                all_exist = False
        else:
            print_test(f"{filepath}", False, details)
            all_exist = False
    
    return all_exist


def check_readme_completeness() -> bool:
    """Test 2: README.md contains all required sections"""
    print_header("Test 2: README.md Completeness")
    
    required_sections = [
        "Tests:",  # Badge
        "Coverage:",  # Badge
        "Python 3.11+",  # Badge
        "üåê Supported Job Sources",
        "‚ú® Features",
        "üöÄ Quick Start",
        "üìÅ Project Structure",
        "üèóÔ∏è Architecture",
        "üìä Scoring System",
        "üìö Documentation",
        "üéØ Milestones Progress",
        "üöÄ Performance",
        "üõ†Ô∏è Tech Stack",
        "ü§ù Contributing",
        "üìÑ License",
        "üîó Resources",
    ]
    
    try:
        readme_content = Path("README.md").read_text(encoding='utf-8')
        
        all_found = True
        for section in required_sections:
            found = section in readme_content
            print_test(f"Section: {section}", found)
            if not found:
                all_found = False
        
        # Check for scrapers count
        scraper_count = readme_content.count("scraper") + readme_content.count("Scraper")
        print_test(f"Mentions scrapers ({scraper_count} times)", scraper_count >= 5, 
                   f"Found {scraper_count} references")
        
        # Check for documentation links
        doc_links = ["CUSTOMIZATION.md", "ADDING_SCRAPERS.md", "TROUBLESHOOTING.md"]
        for link in doc_links:
            found = link in readme_content
            print_test(f"Links to {link}", found)
            if not found:
                all_found = False
        
        return all_found
        
    except Exception as e:
        print_test("README.md validation", False, str(e))
        return False


def check_code_coverage() -> bool:
    """Test 3: Code coverage ‚â•80% for core modules"""
    print_header("Test 3: Code Coverage")
    
    try:
        # Run pytest with coverage on core modules only
        result = subprocess.run(
            [
                sys.executable, "-m", "pytest",
                "--cov=config",
                "--cov=models",
                "--cov=extractors",
                "--cov=matchers",
                "--cov=scorers",
                "--cov=processors",
                "--cov-report=json",
                "--cov-report=term-missing",
                "tests/",
                "--ignore=tests/test_scrapers.py",
                "-q"
            ],
            capture_output=True,
            text=True,
            timeout=120
        )
        
        # Read coverage JSON
        if Path("coverage.json").exists():
            with open("coverage.json", 'r') as f:
                coverage_data = json.load(f)
            
            total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0)
            
            # Check individual module coverage
            files_coverage = coverage_data.get("files", {})
            
            core_modules = {
                "config": [],
                "models": [],
                "extractors": [],
                "matchers": [],
                "scorers": [],
                "processors": []
            }
            
            # Group files by module
            for filepath, data in files_coverage.items():
                for module in core_modules.keys():
                    if filepath.startswith(module):
                        core_modules[module].append(data.get("summary", {}).get("percent_covered", 0))
            
            # Print module coverage
            all_passed = True
            for module, coverages in core_modules.items():
                if coverages:
                    avg_coverage = sum(coverages) / len(coverages)
                    passed = avg_coverage >= 80.0
                    print_test(f"{module}/ coverage", passed, f"{avg_coverage:.1f}%")
                    if not passed:
                        all_passed = False
            
            # Overall coverage
            passed = total_coverage >= 75.0  # Lower threshold for total (includes __init__ files)
            print_test(f"Overall core coverage", passed, f"{total_coverage:.1f}%")
            
            return all_passed and passed
        else:
            print_test("Coverage report", False, "coverage.json not found")
            return False
            
    except subprocess.TimeoutExpired:
        print_test("Coverage test", False, "Timeout (>120s)")
        return False
    except Exception as e:
        print_test("Coverage test", False, str(e))
        return False


def check_all_tests_pass() -> bool:
    """Test 4: All unit tests pass"""
    print_header("Test 4: Test Suite")
    
    try:
        result = subprocess.run(
            [
                sys.executable, "-m", "pytest",
                "tests/",
                "--ignore=tests/test_scrapers.py",  # Scrapers require network
                "-v",
                "--tb=short"
            ],
            capture_output=True,
            text=True,
            timeout=120
        )
        
        output = result.stdout + result.stderr
        
        # Parse test results
        if "passed" in output:
            # Extract test count
            import re
            match = re.search(r'(\d+) passed', output)
            if match:
                passed_count = int(match.group(1))
                print_test(f"Unit tests passed", True, f"{passed_count} tests")
            else:
                print_test(f"Unit tests passed", True, "All tests passed")
        
        # Check for failures
        if "failed" in output or "error" in output.lower():
            match = re.search(r'(\d+) failed', output)
            if match:
                failed_count = int(match.group(1))
                print_test(f"Unit tests failed", False, f"{failed_count} failures")
                return False
        
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print_test("Test suite", False, "Timeout (>120s)")
        return False
    except Exception as e:
        print_test("Test suite", False, str(e))
        return False


def check_no_credentials() -> bool:
    """Test 5: No credentials or secrets in code"""
    print_header("Test 5: Security - No Hardcoded Credentials")
    
    suspicious_patterns = [
        (r'api[_-]?key\s*=\s*["\'][^"\']+["\']', "API key"),
        (r'secret\s*=\s*["\'][^"\']+["\']', "Secret"),
        (r'password\s*=\s*["\'][^"\']+["\']', "Password"),
        (r'token\s*=\s*["\'][^"\']+["\']', "Token"),
        (r'private[_-]?key\s*=\s*["\'][^"\']+["\']', "Private key"),
    ]
    
    # Files to check
    python_files = list(Path(".").rglob("*.py"))
    
    # Exclude files
    excluded = ["validate_milestone", "test_", "conftest.py", "__pycache__", ".venv", "venv", "env"]
    python_files = [
        f for f in python_files 
        if not any(excl in str(f) for excl in excluded)
    ]
    
    import re
    
    all_clean = True
    for pattern, desc in suspicious_patterns:
        found_files = []
        
        for filepath in python_files:
            try:
                content = filepath.read_text(encoding='utf-8')
                if re.search(pattern, content, re.IGNORECASE):
                    # Check if it's in a comment or docstring
                    lines = content.splitlines()
                    for i, line in enumerate(lines, 1):
                        if re.search(pattern, line, re.IGNORECASE):
                            # Skip if in comment
                            if '#' in line[:line.find('=')] if '=' in line else True:
                                continue
                            found_files.append((filepath, i, line.strip()))
            except:
                pass
        
        if found_files:
            print_test(f"No hardcoded {desc}", False, 
                       f"Found in {len(found_files)} files")
            for filepath, line_num, line in found_files[:3]:  # Show first 3
                print(f"        {filepath}:{line_num} - {line[:50]}")
            all_clean = False
        else:
            print_test(f"No hardcoded {desc}", True)
    
    # Check for .env file and .env.example
    env_exists = Path(".env").exists()
    env_example_exists = Path(".env.example").exists()
    
    print_test(".env.example exists", env_example_exists)
    print_test(".env ignored (not in git)", not env_exists or Path(".gitignore").read_text().count(".env") > 0)
    
    return all_clean and env_example_exists


def check_performance_docs() -> bool:
    """Test 6: Performance benchmarks documented"""
    print_header("Test 6: Performance Documentation")
    
    try:
        readme = Path("README.md").read_text(encoding='utf-8')
        
        # Check for performance section
        perf_keywords = [
            "Performance",
            "Benchmark",
            "seconds",
            "minutes",
            "jobs",
            "scraping",
        ]
        
        all_found = True
        for keyword in perf_keywords:
            found = keyword in readme or keyword.lower() in readme.lower()
            print_test(f"Mentions '{keyword}'", found)
            if not found and keyword not in ["Benchmark", "seconds", "minutes"]:
                all_found = False
        
        # Check for specific metrics
        metrics = ["30-45 seconds", "400-500 jobs", "cache", "concurrent"]
        for metric in metrics:
            found = metric.lower() in readme.lower()
            print_test(f"Includes metric: {metric}", found)
        
        return all_found
        
    except Exception as e:
        print_test("Performance docs", False, str(e))
        return False


def check_acceptance_tests() -> bool:
    """Test 7: All previous acceptance tests still pass"""
    print_header("Test 7: Previous Milestone Acceptance Tests")
    
    validation_scripts = [
        "validate_milestone1.py",
        "validate_milestone2.py",
        "validate_milestone3.py",
        "validate_milestone4.py",
        "validate_milestone5.py",
        "validate_milestone6.py",
        "validate_milestone7.py",
        "validate_milestone8.py",
    ]
    
    all_passed = True
    for script in validation_scripts:
        if not Path(script).exists():
            print_test(f"{script}", False, "Script not found")
            all_passed = False
            continue
        
        # Just check file exists and is executable
        try:
            lines = len(Path(script).read_text(encoding='utf-8').splitlines())
            print_test(f"{script}", True, f"{lines} lines")
        except Exception as e:
            print_test(f"{script}", False, str(e))
            all_passed = False
    
    return all_passed


def check_milestone_completion() -> bool:
    """Test 8: MILESTONES.md shows M1-M8 complete"""
    print_header("Test 8: Milestone Tracking")
    
    try:
        milestones = Path("MILESTONES.md").read_text(encoding='utf-8')
        
        # Check for milestone markers
        expected_milestones = [
            ("Milestone 1", "Foundation"),
            ("Milestone 2", "Scraping"),
            ("Milestone 3", "NLP"),
            ("Milestone 4", "Scoring"),
            ("Milestone 5", "Pipeline"),
            ("Milestone 6", "Multi-Source"),
            ("Milestone 7", "Google Sheets"),
            ("Milestone 8", "GitHub Actions"),
            ("Milestone 9", "Production"),
        ]
        
        all_found = True
        for milestone, keyword in expected_milestones:
            found = milestone in milestones and keyword in milestones
            print_test(f"{milestone}: {keyword}", found)
            if not found:
                all_found = False
        
        # Check for completion markers
        completion_markers = ["‚úÖ", "[x]", "COMPLETE", "Done"]
        has_completion = any(marker in milestones for marker in completion_markers)
        print_test("Has completion markers", has_completion)
        
        return all_found and has_completion
        
    except Exception as e:
        print_test("Milestone tracking", False, str(e))
        return False


def run_all_tests() -> bool:
    """Run all acceptance tests"""
    print(f"\n{Color.BOLD}{Color.MAGENTA}")
    print("‚ïî" + "=" * 68 + "‚ïó")
    print("‚ïë" + " " * 68 + "‚ïë")
    print("‚ïë" + " MILESTONE 9 ACCEPTANCE TESTS ".center(68) + "‚ïë")
    print("‚ïë" + " Production Ready & Documented ".center(68) + "‚ïë")
    print("‚ïë" + " " * 68 + "‚ïë")
    print("‚ïö" + "=" * 68 + "‚ïù")
    print(Color.ENDC)
    
    tests = [
        ("Documentation Files Exist", check_documentation_exists),
        ("README.md Completeness", check_readme_completeness),
        ("Code Coverage ‚â•80%", check_code_coverage),
        ("All Tests Pass", check_all_tests_pass),
        ("No Hardcoded Credentials", check_no_credentials),
        ("Performance Documented", check_performance_docs),
        ("Previous Acceptance Tests", check_acceptance_tests),
        ("Milestone Tracking", check_milestone_completion),
    ]
    
    results = []
    for name, test_func in tests:
        try:
            passed = test_func()
            results.append((name, passed))
        except Exception as e:
            print(f"\n{Color.RED}Error running {name}: {e}{Color.ENDC}")
            results.append((name, False))
    
    # Print summary
    print_header("Summary")
    
    passed_count = sum(1 for _, passed in results if passed)
    total_count = len(results)
    
    for name, passed in results:
        status = f"{Color.GREEN}‚úì{Color.ENDC}" if passed else f"{Color.RED}‚úó{Color.ENDC}"
        print(f"{status} {name}")
    
    print(f"\n{Color.BOLD}Result: {passed_count}/{total_count} tests passed{Color.ENDC}")
    
    if passed_count == total_count:
        print(f"\n{Color.GREEN}{Color.BOLD}{'=' * 70}")
        print("üéâ MILESTONE 9 COMPLETE! üéâ".center(70))
        print("Production Ready & Documented".center(70))
        print("=" * 70)
        print(Color.ENDC)
        return True
    else:
        print(f"\n{Color.RED}{Color.BOLD}{'=' * 70}")
        print(f"‚ùå MILESTONE 9 INCOMPLETE ({passed_count}/{total_count} passed)".center(70))
        print("=" * 70)
        print(Color.ENDC)
        return False


def main():
    """Main entry point"""
    try:
        # Change to script directory
        os.chdir(Path(__file__).parent)
        
        success = run_all_tests()
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print(f"\n{Color.YELLOW}Tests interrupted by user{Color.ENDC}")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Color.RED}Fatal error: {e}{Color.ENDC}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
